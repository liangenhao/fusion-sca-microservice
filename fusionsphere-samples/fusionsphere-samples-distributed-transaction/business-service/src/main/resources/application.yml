server:
  port: 10003
spring:
  application:
    name: business-service
  cloud:
    nacos:
      # nacos server 配置文件需开启鉴权：nacos.core.auth.enabled=true
      # 同时nacos server 还需要指定nacos.core.auth.server.identity.key和nacos.core.auth.server.identity.value，
      # 表示当前请求头中包含配置内容，跳过鉴权，用于服务器内部调用不走鉴权认证
      username: nacos
      password: nacos
      discovery:
        # 命名空间ID，用于区分不同环境，默认为public
        #namespace: 911c83a2-bae4-4c78-802a-99dbbbdd66a4 # dev
        server-addr: 127.0.0.1:8848
        # 服务分组，不同的服务可以归类到一个 分组
        group: FUSION_SERVICE_GROUP
        # 虚拟集群，同一个服务下的所有服务实例组成一个默认集群（DEFAULT），可以进一步划分多个虚拟集群
        cluster-name: enhao-local
      config:
        server-addr: 127.0.0.1:8848
        group: FUSION_CONFIG_GROUP # 默认组，可以被 ${spring.config.import} 里?后group覆盖
        refresh-enabled: true # 是否开启动态刷新，默认开启，可以被 ${spring.config.import} 里?后refreshEnabled覆盖
    loadbalancer:
      # 使用 NacosLoadBalancer 作为负载均衡器，替换 spring-cloud-loadbalancer 中默认的的 RoundRobinLoadBalancer（轮训）
      # NacosLoadBalancer 增加对相同虚拟集群和权重（weight）的判断逻辑。主体还是轮训逻辑
      nacos:
        enabled: true # 默认为false
      cache: # 生产环境需添加 caffeine 依赖，使用 caffeine 替换默认缓存
        enabled: true # 默认 true
        capacity: 256 # 缓存容量
        ttl: 35s # 过期时间
        caffeine: # caffeine.spec有值则覆盖 capacity 和 ttl 配置，格式: key1=value1,key2=value2 详见 com.github.benmanes.caffeine.cache.CaffeineSpec.configure
          spec: "initialCapacity=200,expireAfterWrite=10s"
    circuitbreaker:
      sentinel:
        enabled: true # 默认 true，开启 Sentinel 对 Spring Cloud circuitbreaker 标准的实现
    sentinel:
      enabled: true # 默认 true
      eager: true # 默认 false，是否在 Spring 容器启动时就初始化。默认在第一次调用时初始化
      http-method-specify: true # 默认 false，是否添加 http method 前缀
      web-context-unify: true # 默认为 true，流控模式如果是链路，需要配置成 false
      transport:
        port: 8719 # 启动一个 Http Server，该 Server 会与 Sentinel 控制台做交互。如果端口被占用，会自动+1，直到找到未被占用的端口
        dashboard: localhost:8080
        heartbeat-interval-ms: 5000 # 与控制台心跳间隔
      filter:
        enabled: true # 默认 true，是否开启对 Spring webmvc 的拦截器
        url-patterns: # 拦截器拦截路径
          - '/**'
      log:
        dir: logs/csp/${spring.application.name}-${server.port}
      datasource: # 动态数据源，需对 sentinel-dashboard 做改造，详见 https://github.com/liangenhao/Sentinel/tree/master/sentinel-dashboard
        ds-flow:
          nacos:
            username: ${spring.cloud.nacos.username}
            password: ${spring.cloud.nacos.password}
            server-addr: 127.0.0.1:8848
            group-id: SENTINEL_RULES_CONFIG_GROUP # 特别注意: 这里是 group-id，不是 group，和 spring.cloud.nacos.config 的配置 key 是不同的！
            data-type: json # 支持: json、xml、custom，当为 custom 时，指定 converter-class 配置项，写类全路径名
            rule-type: flow # 流控规则
            data-id: ${spring.application.name}-flow-rules
        ds-degrade:
          nacos:
            username: ${spring.cloud.nacos.username}
            password: ${spring.cloud.nacos.password}
            server-addr: 127.0.0.1:8848
            group-id: SENTINEL_RULES_CONFIG_GROUP
            data-type: json
            rule-type: degrade # 熔断规则
            data-id: ${spring.application.name}-degrade-rules
  config:
    # https://sca.aliyun.com/zh-cn/docs/2021.0.5.0/user-guide/nacos/advanced-guide#springcconfigimport-%E5%BC%95%E5%85%A5
    import:
      - optional:nacos:${spring.application.name}.yml?group=FUSION_CONFIG_GROUP&refreshEnabled=true

fusionsphere:
  # sentinel 集群流控
  sentinel-cluster:
    embedded: # 嵌入集群模式
      enabled: true
      cluster-client-config-data-id: ${spring.application.name}-cluster-client-config
      cluster-map-data-id: ${spring.application.name}-cluster-map

feign:
  circuitbreaker: # Spring Cloud Circuitbreaker 标准实现
    enabled: false # 默认 false
  sentinel: # sentinel 的实现
    enabled: true # 默认 false

# https://seata.apache.org/zh-cn/docs/v1.6/user/configurations
seata:
  enabled: true # 是否开启 spring-boot 自动装配，默认 true，如果开启，则会自动配置seata与spring-boot的集成，包括数据源的自动代理以及GlobalTransactionScanner初始化。需依赖 seata-spring-boot-starter.
  application-id: ${spring.application.name} # 默认服务名
  tx-service-group: ${spring.application.name}_tx_group # 事务分组配置，默认 default_tx_group
  enable-auto-data-source-proxy: true # 是否开启数据源自动代理功能，默认 true。TCC 模式不会使用代理对象。
  data-source-proxy-mode: AT # 数据源代理模式，默认 AT
  use-jdk-proxy: false # 是否使用 JDK 代理替换 CGLIB，默认 false
  service:
    # 应用程序事务分组 与 seata-server(TC) 集群映射关系。Map<tx-service-group, TC集群名称>
    # 拿到集群名称程序通过一定的前后缀+集群名称去构造服务名，各配置中心的服务名实现不同（前提是Seata-Server已经完成服务注册，且Seata-Server向注册中心报告cluster名与应用程序（客户端）配置的集群名称一致）
    # 拿到服务名去相应的注册中心去拉取相应服务名的服务列表，获得后端真实的TC服务列表（即Seata-Server集群节点列表）
    # 为什么这么设计，不直接取服务名？这里多了一层获取事务分组到映射集群的配置。这样设计后，事务分组可以作为资源的逻辑隔离单位，出现某集群故障时可以快速failover，只切换对应分组，可以把故障缩减到服务级别，但前提也是你有足够server集群。
    # 注意!!! 程序会通过配置中心去寻找service.vgroupMapping.事务分组配置项，因此该配置需要配到配置中心，否则找不到
    vgroup-mapping: # 若不配置，默认值：default_tx_group:default, my_test_tx_group:default
      ${seata.tx-service-group}: default # 集群名需要与 seata-server 注册到注册中心的 cluster 保持一致
  config: # 需和 seata-server 配置的一致
    type: nacos
    nacos:
      server-addr: 127.0.0.1:8848
      group: SEATA_GROUP
      namespace: ''
      username: nacos
      password: nacos
      dataId: seata.properties
  registry: # 需和 seata-server 配置的一致
    type: nacos
    nacos:
      application: seata-server # 与 seata-server 实际注册服务名一致
      server-addr: 127.0.0.1:8848
      group: SEATA_GROUP # 与 seata-server 实际注册服务名一致
      namespace: ''
      username: nacos
      password: nacos
logging:
  level:
    root: info
#    io.seata: debug
    io.fusion.distributed.transaction.business.rpc: debug
